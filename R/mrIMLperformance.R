#'Wrapper to calculate performance metrics (Mathews correlation coefficient, sensitivity and specificity) for each model for each response variable. 
#'@param yhats A \code{list} is the list generated by mrIMLpredicts
#'@param Model A \code{list}  the model used to generate the yhats object 
#'@param Y  A \code{dataframe} is a response variable data set (species, SNPs etc).
#'@param mode \code{character}'classification' or 'regression' i.e., is the generative model a regression or classification?
#'@examples 
#' \dontrun{
#' ModelPerf <- mrIMLperformance(yhats, Model=model1, Y=Y) }
#'
#'@details Outputs a dataframe of commonly used metric that can be used to compare model performance of classification models. Performance metrics are based on testing data. But MCC is useful (higher numbers = better fit)
#'
#'@export

mrIMLperformance <- function (yhats, Model, Y, mode = "regression"){
  
  n_response <- length(yhats)
  mod_perf <- NULL
  bList <- yhats %>% purrr::map(pluck("last_mod_fit"))
  
  if (mode == "classification") {
    for (i in 1:n_response) {
      met1 <- as.data.frame(bList[[i]]$.metrics)
      roc <- met1$.estimate[2]
      yd <- as.data.frame(bList[[i]]$.predictions)
      mathews <- yardstick::mcc(yd, class, .pred_class)
      mathews <- mathews$.estimate
      sen <- yardstick::sens(yd, class, .pred_class)
      sen <- sen$.estimate
      spe <- yardstick::spec(yd, class, .pred_class)
      ppv_e <- yardstick::ppv(yd, class, .pred_class)
      ppv  <- ppv_e$.estimate
      spe <- spe$.estimate
      mod_name <- class(Model)[1]
      sp <- names(Y[i])
      prev <- sum(Y[i])/nrow(Y)
      mod_perf[[i]] <- c(sp, mod_name, roc, mathews, sen, 
                         spe, ppv,  prev)
    }
    mod1_perf <- do.call(rbind, mod_perf)
    mod1_perf <- as.data.frame(mod1_perf)
    colnames(mod1_perf) <- c("response", "model_name", 
                             "roc_AUC", "mcc", "sensitivity","ppv",
                             "specificity", "prevalence")
    Global_summary <- as.numeric(as.character(unlist(mod1_perf$roc_AUC)))
    Global_summary[is.na(Global_summary)] <- 0
    Global_summary <- mean(Global_summary)
  }
  if (mode == "regression") {
    for (i in 1:n_response) {
      met1 <- as.data.frame(bList[[i]]$.metrics)
      rmse <- met1$.estimate[1]
      rsq <- met1$.estimate[2]
      mod_name <- class(Model)[1]
      sp <- names(Y[i])
      mod_perf[[i]] <- data.frame(sp, mod_name, rmse, rsq)
    }
    mod1_perf <- do.call(rbind, mod_perf)
    mod1_perf <- as.data.frame(mod1_perf)
    colnames(mod1_perf) <- c("response", "model_name", 
                             "rmse", "rsquared")
    Global_summary <- as.numeric(as.character(unlist(mod1_perf$rmse)))
    Global_summary <- mean(Global_summary, na.rm = TRUE)
  }
  return(list(mod1_perf, Global_summary))
}
